{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "QKV（Query-Key-Value）是注意力机制（Attention Mechanism）的核心组件。下面是一个简化的示例，展示如何构造 QKV 并计算注意力分数（Attention Scores）和最终的注意力输出（Attention Output）。\n",
    "\n",
    "假设我们有一个输入序列 \\(\\mathbf{X}\\)，其维度为 \\((N, D)\\)，其中 \\(N\\) 是序列的长度，\\(D\\) 是每个元素的维度。注意力机制会将输入序列投影到三个不同的表示：Query (\\(\\mathbf{Q}\\))，Key (\\(\\mathbf{K}\\)) 和 Value (\\(\\mathbf{V}\\))。这些表示的维度通常为 \\((N, d_k)\\)，其中 \\(d_k\\) 是投影后的维度。\n",
    "\n",
    "### 1. 构造 Q, K, V\n",
    "首先，我们需要定义投影矩阵 \\(\\mathbf{W_Q}\\), \\(\\mathbf{W_K}\\), 和 \\(\\mathbf{W_V}\\)，并通过这些矩阵将输入 \\(\\mathbf{X}\\) 投影到 Q, K, V 空间：\n",
    "\n",
    "\\[ \\mathbf{Q} = \\mathbf{X} \\mathbf{W_Q} \\]\n",
    "\\[ \\mathbf{K} = \\mathbf{X} \\mathbf{W_K} \\]\n",
    "\\[ \\mathbf{V} = \\mathbf{X} \\mathbf{W_V} \\]\n",
    "\n",
    "### 2. 计算注意力分数\n",
    "接下来，我们计算 Query 和 Key 的点积，得到注意力分数：\n",
    "\n",
    "\\[ \\text{Attention Scores} = \\frac{\\mathbf{Q} \\mathbf{K}^T}{\\sqrt{d_k}} \\]\n",
    "\n",
    "这里，我们使用 \\( \\sqrt{d_k} \\) 来进行缩放，从而缓解点积值随维度增加而变大的问题。\n",
    "\n",
    "### 3. 计算注意力权重\n",
    "对注意力分数应用 softmax 函数，得到注意力权重：\n",
    "\n",
    "\\[ \\text{Attention Weights} = \\text{softmax}\\left( \\frac{\\mathbf{Q} \\mathbf{K}^T}{\\sqrt{d_k}} \\right) \\]\n",
    "\n",
    "### 4. 计算注意力输出\n",
    "使用注意力权重对 Value 进行加权求和，得到最终的注意力输出：\n",
    "\n",
    "\\[ \\text{Attention Output} = \\text{Attention Weights} \\cdot \\mathbf{V} \\]\n",
    "\n",
    "下面是一个用 Python 实现上述步骤的简化代码示例：\n"
   ],
   "id": "3fd87ae10557976"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-03T10:31:52.473268Z",
     "start_time": "2024-08-03T10:31:52.288511Z"
    }
   },
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# 输入序列，假设长度为 3，每个元素的维度为 4\n",
    "X = np.random.rand(3, 4)\n",
    "\n",
    "# 投影矩阵，假设投影后的维度为 2\n",
    "W_Q = np.random.rand(4, 2)\n",
    "W_K = np.random.rand(4, 2)\n",
    "W_V = np.random.rand(4, 2)\n",
    "\n",
    "# 构造 Q, K, V\n",
    "Q = np.dot(X, W_Q)\n",
    "K = np.dot(X, W_K)\n",
    "V = np.dot(X, W_V)\n",
    "\n",
    "# 计算注意力分数\n",
    "dk = Q.shape[-1]\n",
    "attention_scores = np.dot(Q, K.T) / np.sqrt(dk)\n",
    "\n",
    "# 计算注意力权重\n",
    "attention_weights = np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=-1, keepdims=True)\n",
    "\n",
    "# 计算注意力输出\n",
    "attention_output = np.dot(attention_weights, V)\n",
    "\n",
    "print(\"Attention Output:\\n\", attention_output)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Output:\n",
      " [[0.5509069  0.95314136]\n",
      " [0.5579653  0.94560659]\n",
      " [0.55115609 0.95267676]]\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "这个示例展示了如何构造 Q, K, V 并计算注意力分数、注意力权重和最终的注意力输出。你可以根据具体需求调整输入维度和投影维度。\n",
   "id": "be9aaabaf9c7af4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "多头注意力机制（Multi-Head Attention）是在单头注意力机制的基础上，通过引入多个并行的注意力头来捕获更多的特征信息。每个头都有自己的 Query、Key 和 Value 投影矩阵，最后将所有头的输出拼接在一起，并通过一个线性变换得到最终的输出。\n",
    "\n",
    "下面是实现多头注意力机制的代码示例：\n",
    "\n",
    "\n",
    "### 代码说明：\n",
    "\n",
    "1. **初始化投影矩阵**：\n",
    "    - 为每个头初始化独立的 \\(\\mathbf{W_Q}\\), \\(\\mathbf{W_K}\\), 和 \\(\\mathbf{W_V}\\) 矩阵。\n",
    "    - 初始化一个输出投影矩阵 \\(\\mathbf{W_O}\\)。\n",
    "\n",
    "2. **计算每个头的注意力输出**：\n",
    "    - 对于每个头，使用对应的投影矩阵将输入 \\(\\mathbf{X}\\) 转换为 Q, K, V。\n",
    "    - 计算注意力分数和权重。\n",
    "    - 使用注意力权重对 V 进行加权求和，得到每个头的注意力输出。\n",
    "\n",
    "3. **拼接和输出投影**：\n",
    "    - 将所有头的输出拼接在一起。\n",
    "    - 使用输出投影矩阵 \\(\\mathbf{W_O}\\) 对拼接结果进行线性变换，得到最终的多头注意力输出。\n",
    "\n",
    "这个示例展示了如何在单头注意力机制的基础上实现多头注意力机制，从而捕获输入序列中的更多特征信息。"
   ],
   "id": "66e6bb391b4b4099"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T10:33:52.272107Z",
     "start_time": "2024-08-03T10:33:52.259505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def multi_head_attention(X, num_heads, d_model, d_k):\n",
    "    # 输入序列维度 (N, D)\n",
    "    N, D = X.shape\n",
    "\n",
    "    # 初始化投影矩阵\n",
    "    W_Q = [np.random.rand(D, d_k) for _ in range(num_heads)]\n",
    "    W_K = [np.random.rand(D, d_k) for _ in range(num_heads)]\n",
    "    W_V = [np.random.rand(D, d_k) for _ in range(num_heads)]\n",
    "    W_O = np.random.rand(num_heads * d_k, d_model)  # 输出投影矩阵\n",
    "\n",
    "    heads = []\n",
    "    for i in range(num_heads):\n",
    "        # 构造 Q, K, V\n",
    "        Q = np.dot(X, W_Q[i])\n",
    "        K = np.dot(X, W_K[i])\n",
    "        V = np.dot(X, W_V[i])\n",
    "\n",
    "        # 计算注意力分数\n",
    "        attention_scores = np.dot(Q, K.T) / np.sqrt(d_k)\n",
    "\n",
    "        # 计算注意力权重\n",
    "        attention_weights = np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=-1, keepdims=True)\n",
    "\n",
    "        # 计算注意力输出\n",
    "        attention_output = np.dot(attention_weights, V)\n",
    "        heads.append(attention_output)\n",
    "\n",
    "    # 拼接所有头的输出\n",
    "    concatenated_heads = np.concatenate(heads, axis=-1)\n",
    "\n",
    "    # 输出投影\n",
    "    output = np.dot(concatenated_heads, W_O)\n",
    "\n",
    "    return output\n",
    "\n",
    "# 示例参数\n",
    "num_heads = 4\n",
    "d_model = 8\n",
    "d_k = 2\n",
    "\n",
    "# 输入序列，假设长度为 3，每个元素的维度为 4\n",
    "X = np.random.rand(3, 4)\n",
    "\n",
    "# 计算多头注意力输出\n",
    "output = multi_head_attention(X, num_heads, d_model, d_k)\n",
    "print(\"Multi-Head Attention Output:\\n\", output)"
   ],
   "id": "f9dc5354ac5ba4e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Head Attention Output:\n",
      " [[4.02793075 4.22061178 4.25755962 2.62747021 3.2275044  3.68996179\n",
      "  4.20277687 4.15890077]\n",
      " [4.02844089 4.22099713 4.25890455 2.62295077 3.22745622 3.68704788\n",
      "  4.21223231 4.14318212]\n",
      " [4.02449526 4.2168     4.25561264 2.62051565 3.22300288 3.68407833\n",
      "  4.20427625 4.14667819]]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "PyTorch 版本",
   "id": "6a39c010e31ce366"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T10:49:55.826260Z",
     "start_time": "2024-08-03T10:49:55.819839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 参数\n",
    "seq_length = 8\n",
    "kv_channels = 6\n",
    "sq = sk = seq_length\n",
    "bq = bk = 1\n",
    "hq = 10\n",
    "hk = 2\n",
    "dq = dk = kv_channels\n",
    "\n",
    "# 随机初始化 Q, K, V\n",
    "Q = torch.rand(sq, bq, hq, dq)\n",
    "K = torch.rand(sk, bk, hk, dk)\n",
    "V = torch.rand(sk, bk, hk, dk)\n",
    "\n",
    "# 计算注意力分数\n",
    "attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(dk, dtype=torch.float32))\n",
    "\n",
    "# 计算注意力权重\n",
    "attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "# 计算注意力输出\n",
    "attention_output = torch.matmul(attention_weights, V)\n",
    "\n",
    "print(\"Attention Output:\\n\", attention_output)\n"
   ],
   "id": "4e6a724fedc5dad3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Output:\n",
      " tensor([[[[0.2728, 0.4244, 0.7367, 0.8124, 0.7896, 0.3257],\n",
      "          [0.2554, 0.4438, 0.7514, 0.8165, 0.8025, 0.3187],\n",
      "          [0.2886, 0.4069, 0.7235, 0.8088, 0.7780, 0.3320],\n",
      "          [0.2562, 0.4429, 0.7507, 0.8163, 0.8019, 0.3190],\n",
      "          [0.2661, 0.4319, 0.7424, 0.8140, 0.7946, 0.3230],\n",
      "          [0.2393, 0.4618, 0.7650, 0.8202, 0.8145, 0.3122],\n",
      "          [0.2788, 0.4178, 0.7318, 0.8111, 0.7852, 0.3281],\n",
      "          [0.2729, 0.4244, 0.7367, 0.8124, 0.7896, 0.3257],\n",
      "          [0.2471, 0.4530, 0.7584, 0.8184, 0.8087, 0.3154],\n",
      "          [0.2901, 0.4052, 0.7222, 0.8084, 0.7768, 0.3326]]],\n",
      "\n",
      "\n",
      "        [[[0.4598, 0.1882, 0.9095, 0.2584, 0.1334, 0.8120],\n",
      "          [0.4573, 0.1902, 0.9079, 0.2493, 0.1371, 0.8140],\n",
      "          [0.4545, 0.1925, 0.9061, 0.2389, 0.1412, 0.8163],\n",
      "          [0.4643, 0.1845, 0.9123, 0.2749, 0.1269, 0.8084],\n",
      "          [0.4605, 0.1876, 0.9099, 0.2609, 0.1325, 0.8115],\n",
      "          [0.4581, 0.1896, 0.9083, 0.2520, 0.1360, 0.8135],\n",
      "          [0.4593, 0.1886, 0.9091, 0.2563, 0.1343, 0.8125],\n",
      "          [0.4565, 0.1909, 0.9073, 0.2462, 0.1383, 0.8147],\n",
      "          [0.4564, 0.1909, 0.9073, 0.2460, 0.1384, 0.8148],\n",
      "          [0.4539, 0.1929, 0.9057, 0.2369, 0.1420, 0.8168]]],\n",
      "\n",
      "\n",
      "        [[[0.1945, 0.5006, 0.3505, 0.3823, 0.6752, 0.7983],\n",
      "          [0.1891, 0.5083, 0.3555, 0.3898, 0.6748, 0.8016],\n",
      "          [0.1893, 0.5080, 0.3553, 0.3895, 0.6749, 0.8015],\n",
      "          [0.2017, 0.4903, 0.3439, 0.3724, 0.6758, 0.7939],\n",
      "          [0.2094, 0.4793, 0.3368, 0.3618, 0.6763, 0.7892],\n",
      "          [0.2117, 0.4760, 0.3347, 0.3587, 0.6765, 0.7878],\n",
      "          [0.2054, 0.4851, 0.3405, 0.3674, 0.6760, 0.7917],\n",
      "          [0.1975, 0.4964, 0.3478, 0.3783, 0.6755, 0.7965],\n",
      "          [0.1989, 0.4944, 0.3465, 0.3764, 0.6756, 0.7957],\n",
      "          [0.2078, 0.4816, 0.3383, 0.3640, 0.6762, 0.7902]]],\n",
      "\n",
      "\n",
      "        [[[0.7655, 0.5346, 0.5142, 0.3185, 0.6847, 0.4442],\n",
      "          [0.7606, 0.5753, 0.5201, 0.2924, 0.7323, 0.3786],\n",
      "          [0.7628, 0.5571, 0.5175, 0.3041, 0.7110, 0.4080],\n",
      "          [0.7653, 0.5359, 0.5144, 0.3177, 0.6862, 0.4421],\n",
      "          [0.7641, 0.5461, 0.5159, 0.3112, 0.6982, 0.4257],\n",
      "          [0.7647, 0.5414, 0.5152, 0.3142, 0.6926, 0.4334],\n",
      "          [0.7654, 0.5351, 0.5143, 0.3182, 0.6853, 0.4435],\n",
      "          [0.7637, 0.5498, 0.5164, 0.3088, 0.7024, 0.4199],\n",
      "          [0.7643, 0.5441, 0.5156, 0.3125, 0.6957, 0.4290],\n",
      "          [0.7645, 0.5429, 0.5154, 0.3132, 0.6944, 0.4309]]],\n",
      "\n",
      "\n",
      "        [[[0.6316, 0.0985, 0.3013, 0.7032, 0.8526, 0.4066],\n",
      "          [0.6276, 0.0977, 0.2968, 0.7047, 0.8546, 0.4029],\n",
      "          [0.6274, 0.0976, 0.2965, 0.7048, 0.8547, 0.4026],\n",
      "          [0.6462, 0.1015, 0.3178, 0.6976, 0.8454, 0.4205],\n",
      "          [0.6117, 0.0944, 0.2789, 0.7107, 0.8624, 0.3877],\n",
      "          [0.6548, 0.1032, 0.3274, 0.6944, 0.8412, 0.4286],\n",
      "          [0.6178, 0.0957, 0.2858, 0.7084, 0.8594, 0.3935],\n",
      "          [0.6341, 0.0990, 0.3041, 0.7022, 0.8514, 0.4090],\n",
      "          [0.6460, 0.1014, 0.3174, 0.6977, 0.8455, 0.4202],\n",
      "          [0.6417, 0.1006, 0.3127, 0.6993, 0.8476, 0.4162]]],\n",
      "\n",
      "\n",
      "        [[[0.3672, 0.6559, 0.6121, 0.3880, 0.4839, 0.6510],\n",
      "          [0.3655, 0.6416, 0.6164, 0.3858, 0.5001, 0.6695],\n",
      "          [0.3665, 0.6500, 0.6139, 0.3871, 0.4906, 0.6586],\n",
      "          [0.3662, 0.6474, 0.6147, 0.3867, 0.4935, 0.6620],\n",
      "          [0.3704, 0.6824, 0.6043, 0.3923, 0.4539, 0.6166],\n",
      "          [0.3636, 0.6258, 0.6211, 0.3833, 0.5179, 0.6900],\n",
      "          [0.3677, 0.6597, 0.6110, 0.3886, 0.4796, 0.6460],\n",
      "          [0.3655, 0.6417, 0.6164, 0.3858, 0.5000, 0.6694],\n",
      "          [0.3673, 0.6562, 0.6120, 0.3881, 0.4835, 0.6505],\n",
      "          [0.3641, 0.6301, 0.6198, 0.3839, 0.5131, 0.6844]]],\n",
      "\n",
      "\n",
      "        [[[0.4745, 0.3816, 0.3383, 0.6525, 0.8905, 0.5434],\n",
      "          [0.4431, 0.4027, 0.3162, 0.6704, 0.8877, 0.5343],\n",
      "          [0.4589, 0.3921, 0.3273, 0.6614, 0.8891, 0.5389],\n",
      "          [0.4782, 0.3791, 0.3410, 0.6504, 0.8908, 0.5445],\n",
      "          [0.4588, 0.3921, 0.3273, 0.6614, 0.8891, 0.5388],\n",
      "          [0.4437, 0.4023, 0.3166, 0.6701, 0.8877, 0.5345],\n",
      "          [0.4596, 0.3916, 0.3278, 0.6610, 0.8892, 0.5391],\n",
      "          [0.4477, 0.3996, 0.3194, 0.6678, 0.8881, 0.5356],\n",
      "          [0.4771, 0.3798, 0.3402, 0.6510, 0.8907, 0.5442],\n",
      "          [0.4722, 0.3832, 0.3367, 0.6538, 0.8903, 0.5427]]],\n",
      "\n",
      "\n",
      "        [[[0.4690, 0.8762, 0.6984, 0.5926, 0.2712, 0.3882],\n",
      "          [0.4641, 0.8702, 0.7061, 0.6223, 0.2810, 0.3624],\n",
      "          [0.4627, 0.8685, 0.7083, 0.6308, 0.2838, 0.3551],\n",
      "          [0.4692, 0.8765, 0.6981, 0.5915, 0.2709, 0.3891],\n",
      "          [0.4629, 0.8688, 0.7079, 0.6293, 0.2833, 0.3564],\n",
      "          [0.4647, 0.8710, 0.7051, 0.6184, 0.2797, 0.3658],\n",
      "          [0.4665, 0.8731, 0.7024, 0.6079, 0.2763, 0.3749],\n",
      "          [0.4641, 0.8703, 0.7061, 0.6221, 0.2810, 0.3626],\n",
      "          [0.4674, 0.8743, 0.7009, 0.6021, 0.2744, 0.3799],\n",
      "          [0.4638, 0.8699, 0.7065, 0.6238, 0.2815, 0.3611]]]])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "edbe0af7897e37f1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
