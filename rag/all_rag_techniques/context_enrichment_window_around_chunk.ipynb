{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 用于文档检索的上下文丰富窗口技术\n",
    "\n",
    "## 概述\n",
    "\n",
    "此代码实现了一种用于向量数据库中文档检索的上下文丰富窗口技术。它通过**在每个检索到的块周围添加上下文**，增强了标准的检索过程，提高了返回信息的连贯性和完整性。\n",
    "\n",
    "## 动机\n",
    "\n",
    "传统的向量搜索通常返回孤立的文本块，这些文本块可能缺乏全面理解所需的上下文。这种方法旨在通过包括邻近的文本块来提供检索信息的更全面视图。\n",
    "\n",
    "## 关键组件\n",
    "\n",
    "1. PDF处理和文本分块\n",
    "2. 使用FAISS和OpenAI嵌入创建向量存储\n",
    "3. 自定义检索函数带上下文窗口\n",
    "4. 标准检索与上下文丰富检索的比较\n",
    "\n",
    "## 方法细节\n",
    "\n",
    "### 文档预处理\n",
    "\n",
    "1. 读取PDF并转换为字符串。\n",
    "2. 将文本分割成有重叠的块，每个块标记其索引。\n",
    "\n",
    "### 向量存储创建\n",
    "\n",
    "1. 使用OpenAI嵌入创建块的向量表示。\n",
    "2. 从这些嵌入创建FAISS向量存储。\n",
    "\n",
    "### 上下文丰富检索\n",
    "\n",
    "1. `retrieve_with_context_overlap` 函数执行以下步骤：\n",
    "   - 根据查询检索相关块\n",
    "   - 对于每个相关块，获取邻近块\n",
    "   - 连接块，考虑重叠\n",
    "   - 返回每个相关块的扩展上下文\n",
    "\n",
    "### 检索比较\n",
    "\n",
    "jupyter 包括一个用于比较标准检索与上下文丰富方法的部分。\n",
    "\n",
    "## 这种方法的好处\n",
    "\n",
    "1. 提供更连贯和上下文丰富的结果\n",
    "2. 保持向量搜索的优势，同时减轻其倾向于返回孤立文本片段的倾向\n",
    "3. 允许灵活调整上下文窗口大小\n",
    "\n",
    "## 结论\n",
    "\n",
    "这种上下文丰富窗口技术为提高基于向量的文档搜索系统中检索信息的质量提供了有希望的方法。通过提供周围上下文，它有助于保持检索信息的连贯性和完整性，可能在诸如问答等下游任务中导致更好的理解和更准确的响应。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "\n",
    "<img src=\"../images/context_enrichment_window.svg\" alt=\"context enrichment window\" style=\"width:70%; height:auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and environment variables"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T06:35:16.181802Z",
     "start_time": "2024-08-25T06:35:16.174923Z"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) # Add the parent directory to the path sicnce we work with notebooks\n",
    "from rag.helper_functions import *\n",
    "from rag.evaluation.evalute_rag import *\n",
    "\n",
    "# Set the OpenAI API key environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define path to PDF"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T06:35:20.795490Z",
     "start_time": "2024-08-25T06:35:20.792873Z"
    }
   },
   "source": "path = \"../data/Understanding_Climate_Change.pdf\"",
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read PDF to string"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T06:35:23.990917Z",
     "start_time": "2024-08-25T06:35:23.931852Z"
    }
   },
   "source": "content = read_pdf_to_string(path)",
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 将文本分割成块，并带有块的顺序索引元数据的函数"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T06:35:53.655593Z",
     "start_time": "2024-08-25T06:35:53.651719Z"
    }
   },
   "source": [
    "def split_text_to_chunks_with_indices(text: str, chunk_size: int, chunk_overlap: int) -> List[Document]:\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(Document(page_content=chunk, metadata={\"index\": len(chunks), \"text\": text}))\n",
    "        start += chunk_size - chunk_overlap\n",
    "    return chunks"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 相应地拆分我们的文档"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T06:37:31.506104Z",
     "start_time": "2024-08-25T06:37:31.501105Z"
    }
   },
   "source": [
    "chunks_size = 400\n",
    "chunk_overlap = 200\n",
    "docs = split_text_to_chunks_with_indices(content, chunks_size, chunk_overlap)\n",
    "print(len(docs))\n",
    "print(len(docs[0].page_content))\n",
    "print(len(docs[-1].page_content))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72561\n",
      "363\n",
      "400\n",
      "161\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 创建向量存储和检索器"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T06:38:09.662182Z",
     "start_time": "2024-08-25T06:38:06.060364Z"
    }
   },
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "chunks_query_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 从向量存储中按原始顺序绘制第k个块"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T06:42:55.405683Z",
     "start_time": "2024-08-25T06:42:55.393458Z"
    }
   },
   "source": [
    "def get_chunk_by_index(vectorstore, target_index: int) -> Document:\n",
    "    \"\"\"\n",
    "    根据元数据中的索引从向量存储中检索一块数据。\n",
    "    \n",
    "    参数:\n",
    "    vectorstore (VectorStore): 包含数据块的向量存储。\n",
    "    target_index (int): 要检索的数据块的索引。\n",
    "    \n",
    "    返回值:\n",
    "    Optional[Document]: 检索到的数据块作为Document对象返回，如果没有找到则返回None。\n",
    "    \"\"\"\n",
    "    # 这是一个简化版本。在实践中，你可能需要一个更有效的方法\n",
    "    # 根据索引检索数据块，这取决于你的向量存储实现方式。\n",
    "\n",
    "    all_docs = vectorstore.similarity_search(\"\", k=vectorstore.index.ntotal)\n",
    "    for doc in all_docs:\n",
    "        if doc.metadata.get('index') == target_index:\n",
    "            return doc\n",
    "    return None"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the function"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T06:43:04.205763Z",
     "start_time": "2024-08-25T06:43:03.289738Z"
    }
   },
   "source": [
    "chunk = get_chunk_by_index(vectorstore, 0)\n",
    "print(chunk.page_content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understanding Climate Change \n",
      "Chapter 1: Introduction to Climate Change \n",
      "Climate change refers to significant, long-term changes in the global climate. The term \n",
      "\"global climate\" encompasses the planet's overall weather patterns, including temperature, \n",
      "precipitation, and wind patterns, over an extended period. Over the past century, human \n",
      "activities, particularly the burning of fossil fuels and \n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 基于语义相似性从向量存储中检索，然后将每个检索到的块在前后填充其num_neighbors，并考虑块重叠以在其周围构造一个有意义的宽窗口"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T06:50:42.007950Z",
     "start_time": "2024-08-25T06:50:41.999625Z"
    }
   },
   "source": [
    "def retrieve_with_context_overlap(vectorstore, retriever, query: str, num_neighbors: int = 1, chunk_size: int = 200, chunk_overlap: int = 20) -> List[str]:\n",
    "    \"\"\"\n",
    "    根据查询检索数据块，然后获取相邻的数据块并将它们连接起来，\n",
    "    同时考虑重叠部分并正确索引。\n",
    "\n",
    "    参数:\n",
    "    vectorstore (VectorStore): 包含数据块的向量存储。\n",
    "    retriever: 获取相关文档的检索器对象。\n",
    "    query (str): 搜索相关数据块的查询语句。\n",
    "    num_neighbors (int): 在每个相关数据块之前和之后检索的数据块数量。\n",
    "    chunk_size (int): 最初分割时每个数据块的大小。\n",
    "    chunk_overlap (int): 最初分割时数据块之间的重叠部分。\n",
    "\n",
    "    返回值:\n",
    "    List[str]: 连接后的数据块序列列表，每个序列都以一个相关数据块为中心。\n",
    "    \"\"\"\n",
    "\n",
    "    relevant_chunks = retriever.get_relevant_documents(query)\n",
    "    result_sequences = []\n",
    "\n",
    "    for chunk in relevant_chunks:\n",
    "        current_index = chunk.metadata.get('index')\n",
    "        if current_index is None:\n",
    "            continue\n",
    "\n",
    "        # Determine the range of chunks to retrieve\n",
    "        start_index = max(0, current_index - num_neighbors)\n",
    "        end_index = current_index + num_neighbors + 1  # +1 because range is exclusive at the end\n",
    "\n",
    "        # Retrieve all chunks in the range\n",
    "        neighbor_chunks = []\n",
    "        for i in range(start_index, end_index):\n",
    "            neighbor_chunk = get_chunk_by_index(vectorstore, i)\n",
    "            if neighbor_chunk:\n",
    "                neighbor_chunks.append(neighbor_chunk)\n",
    "\n",
    "        # Sort chunks by their index to ensure correct order\n",
    "        neighbor_chunks.sort(key=lambda x: x.metadata.get('index', 0))\n",
    "\n",
    "        # Concatenate chunks, accounting for overlap\n",
    "        concatenated_text = neighbor_chunks[0].page_content\n",
    "        for i in range(1, len(neighbor_chunks)):\n",
    "            current_chunk = neighbor_chunks[i].page_content\n",
    "            overlap_start = max(0, len(concatenated_text) - chunk_overlap)\n",
    "            concatenated_text = concatenated_text[:overlap_start] + current_chunk\n",
    "\n",
    "        result_sequences.append(concatenated_text)\n",
    "\n",
    "    return result_sequences"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 比较常规检索和上下文窗口检索"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T06:50:52.699459Z",
     "start_time": "2024-08-25T06:50:48.801527Z"
    }
   },
   "source": [
    "# Baseline approach\n",
    "query = \"Explain the role of deforestation and fossil fuels in climate change.\"\n",
    "baseline_chunk = chunks_query_retriever.get_relevant_documents(query, k=1)\n",
    "# Focused context enrichment approach\n",
    "enriched_chunks = retrieve_with_context_overlap(\n",
    "    vectorstore,\n",
    "    chunks_query_retriever,\n",
    "    query,\n",
    "    num_neighbors=1,\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "print(\"Baseline Chunk:\")\n",
    "print(baseline_chunk[0].page_content)\n",
    "print(\"\\nEnriched Chunks:\")\n",
    "print(enriched_chunks[0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Chunk:\n",
      "ntribute \n",
      "to climate change. These forests are vital for regulating the Earth's climate and supporting \n",
      "indigenous communities and wildlife. \n",
      "Agriculture \n",
      "Agriculture contributes to climate change through methane emissions from livestock, rice \n",
      "paddies, and the use of synthetic fertilizers. Methane is a potent greenhouse gas with a much \n",
      "higher heat-trapping capability than CO2, albeit in smaller \n",
      "\n",
      "Enriched Chunks:\n",
      "n. \n",
      "Boreal Forests \n",
      "Boreal forests, found in the northern regions of North America, Europe, and Asia, also play a \n",
      "crucial role in sequestering carbon. Logging and land-use changes in these regions contribute \n",
      "to climate change. These forests are vital for regulating the Earth's climate and supporting \n",
      "indigenous communities and wildlife. \n",
      "Agriculture \n",
      "Agriculture contributes to climate change through methane emissions from livestock, rice \n",
      "paddies, and the use of synthetic fertilizers. Methane is a potent greenhouse gas with a much \n",
      "higher heat-trapping capability than CO2, albeit in smaller quantities. \n",
      "Livestock Emissions \n",
      "Ruminant animals, such as cows and sheep, produce methane during digestion. Manure \n",
      "management practices also contribute to methane and nitrous oxide emissions. Innov\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 这个示例展示了附加上下文窗口的优越性"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T06:51:40.498887Z",
     "start_time": "2024-08-25T06:51:36.424708Z"
    }
   },
   "source": [
    "\n",
    "document_content = \"\"\"\n",
    "Artificial Intelligence (AI) has a rich history dating back to the mid-20th century. The term \"Artificial Intelligence\" was coined in 1956 at the Dartmouth Conference, marking the field's official beginning.\n",
    "\n",
    "In the 1950s and 1960s, AI research focused on symbolic methods and problem-solving. The Logic Theorist, created in 1955 by Allen Newell and Herbert A. Simon, is often considered the first AI program.\n",
    "\n",
    "The 1960s saw the development of expert systems, which used predefined rules to solve complex problems. DENDRAL, created in 1965, was one of the first expert systems, designed to analyze chemical compounds.\n",
    "\n",
    "However, the 1970s brought the first \"AI Winter,\" a period of reduced funding and interest in AI research, largely due to overpromised capabilities and underdelivered results.\n",
    "\n",
    "The 1980s saw a resurgence with the popularization of expert systems in corporations. The Japanese government's Fifth Generation Computer Project also spurred increased investment in AI research globally.\n",
    "\n",
    "Neural networks gained prominence in the 1980s and 1990s. The backpropagation algorithm, although discovered earlier, became widely used for training multi-layer networks during this time.\n",
    "\n",
    "The late 1990s and 2000s marked the rise of machine learning approaches. Support Vector Machines (SVMs) and Random Forests became popular for various classification and regression tasks.\n",
    "\n",
    "Deep Learning, a subset of machine learning using neural networks with many layers, began to show promising results in the early 2010s. The breakthrough came in 2012 when a deep neural network significantly outperformed other machine learning methods in the ImageNet competition.\n",
    "\n",
    "Since then, deep learning has revolutionized many AI applications, including image and speech recognition, natural language processing, and game playing. In 2016, Google's AlphaGo defeated a world champion Go player, a landmark achievement in AI.\n",
    "\n",
    "The current era of AI is characterized by the integration of deep learning with other AI techniques, the development of more efficient and powerful hardware, and the ethical considerations surrounding AI deployment.\n",
    "\n",
    "Transformers, introduced in 2017, have become a dominant architecture in natural language processing, enabling models like GPT (Generative Pre-trained Transformer) to generate human-like text.\n",
    "\n",
    "As AI continues to evolve, new challenges and opportunities arise. Explainable AI, robust and fair machine learning, and artificial general intelligence (AGI) are among the key areas of current and future research in the field.\n",
    "\"\"\"\n",
    "\n",
    "chunks_size = 250\n",
    "chunk_overlap = 20\n",
    "document_chunks = split_text_to_chunks_with_indices(document_content, chunks_size, chunk_overlap)\n",
    "document_vectorstore = FAISS.from_documents(document_chunks, embeddings)\n",
    "document_retriever = document_vectorstore.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "query = \"When did deep learning become prominent in AI?\"\n",
    "context = document_retriever.get_relevant_documents(query)\n",
    "context_pages_content = [doc.page_content for doc in context]\n",
    "\n",
    "print(\"Regular retrieval:\\n\")\n",
    "show_context(context_pages_content)\n",
    "\n",
    "sequences = retrieve_with_context_overlap(document_vectorstore, document_retriever, query, num_neighbors=1)\n",
    "print(\"\\nRetrieval with context enrichment:\\n\")\n",
    "show_context(sequences)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular retrieval:\n",
      "\n",
      "Context 1:\n",
      "\n",
      "Deep Learning, a subset of machine learning using neural networks with many layers, began to show promising results in the early 2010s. The breakthrough came in 2012 when a deep neural network significantly outperformed other machine learning method\n",
      "\n",
      "\n",
      "\n",
      "Retrieval with context enrichment:\n",
      "\n",
      "Context 1:\n",
      "ng multi-layer networks during this time.\n",
      "\n",
      "The late 1990s and 2000s marked the rise of machine learning approaches. Support Vector Machines (SVMs) and Random Forests became popular for various classification and regression tasks.\n",
      "\n",
      "Deep Learning, a subset of machine learning using neural networks with many layers, began to show promising results in the early 2010s. The breakthrough came in 2012 when a deep neural network significantly outperformed other machine learning methods in the ImageNet competition.\n",
      "\n",
      "Since then, deep learning has revolutionized many AI applications, including image and speech recognition, natural language processing, and game playing. In 2016, Google's AlphaGo defeated a world c\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 30
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
