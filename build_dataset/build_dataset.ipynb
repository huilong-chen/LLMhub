{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "聚合下面这四个开源指令微调数据集，从中得到共287k数据。\n",
    "- Magicoder-OSS-Instruct: https://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K\n",
    "- Python code subset of ShareGPT: https://huggingface.co/datasets/ajibawa-2023/Python-Code-23k-ShareGPT\n",
    "- Magicoder-Evol-Instruct: https://huggingface.co/datasets/ise-uiuc/Magicoder-Evol-Instruct-110K\n",
    "- Evol-Instruct-Code: https://huggingface.co/datasets/nickrosh/Evol-Instruct-Code-80k-v1"
   ],
   "id": "db980c439c02c413"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "ds1 = load_dataset(\"/Users/baoshui/data/Magicoder-OSS-Instruct-75K\")\n",
    "ds2 = load_dataset(\"/Users/baoshui/data/Python-Code-23k-ShareGPT\")\n",
    "ds3 = load_dataset(\"/Users/baoshui/data/Magicoder-Evol-Instruct-110K\")\n",
    "ds4 = load_dataset(\"/Users/baoshui/data/Evol-Instruct-Code-80k-v1\")\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "分别处理上面四个数据集，保存到一个新的jsonl文件中，文件路径 /Users/baoshui/data/CodeMaster/code_master.jsonl，数据格式同一处理成：\n",
    "```json\n",
    "{\n",
    "    \"query\": \"\",\n",
    "    \"answer\": \"\",\n",
    "    \"resource\": \"\",\n",
    "    \"lang\": \"\"\n",
    "}\n",
    "```"
   ],
   "id": "57dea729aaf9e79e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "target_jsonl = \"/Users/baoshui/data/CodeMaster/code_master.jsonl\"",
   "id": "4a957c1228f43906",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(\"/Users/baoshui/data/Magicoder-OSS-Instruct-75K/data-oss_instruct-decontaminated.jsonl\", 'r', encoding='utf-8') as input_file, open(target_jsonl, 'a', encoding='utf-8') as output_file:\n",
    "    for line in input_file:\n",
    "        data = json.loads(line)\n",
    "        new_data = {\n",
    "            \"query\": data[\"problem\"],\n",
    "            \"answer\": data[\"solution\"],\n",
    "            \"resource\": \"Magicoder-OSS-Instruct-75K\",\n",
    "            \"lang\": data[\"lang\"]\n",
    "        }\n",
    "        output_file.write(json.dumps(new_data, ensure_ascii=False) + '\\n')\n"
   ],
   "id": "baff8da7302e882b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(\"/Users/baoshui/data/Python-Code-23k-ShareGPT/Python-Code-23k-ShareGPT.json\", 'r', encoding='utf-8') as input_file, open(target_jsonl, 'a', encoding='utf-8') as output_file:\n",
    "    \n",
    "    data = json.load(input_file)\n",
    "    for d in data:\n",
    "        new_data = {\n",
    "            \"query\": d[\"conversations\"][0][\"value\"],\n",
    "            \"answer\": d[\"conversations\"][1][\"value\"],\n",
    "            \"resource\": \"Python-Code-23k-ShareGPT\",\n",
    "            \"lang\": \"python\"\n",
    "        }\n",
    "        output_file.write(json.dumps(new_data, ensure_ascii=False) + '\\n')"
   ],
   "id": "6c644fef7f13848f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(\"/Users/baoshui/data/Magicoder-Evol-Instruct-110K/data-evol_instruct-decontaminated.jsonl\", 'r', encoding='utf-8') as input_file, open(target_jsonl, 'a', encoding='utf-8') as output_file:\n",
    "    batch = []\n",
    "    for line in input_file:\n",
    "        d = json.loads(line)\n",
    "        new_data = {\n",
    "            \"query\": d[\"instruction\"],\n",
    "            \"answer\": d[\"response\"],\n",
    "            \"resource\": \"Magicoder-Evol-Instruct-110K\",\n",
    "            \"lang\": \"python\"\n",
    "        }\n",
    "        batch.append(new_data)\n",
    "        if len(batch) >= 1000:\n",
    "            for item in batch:\n",
    "                output_file.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "            batch = []\n",
    "    if batch:\n",
    "        for item in batch:\n",
    "            output_file.write(json.dumps(item, ensure_ascii=False) + '\\n')"
   ],
   "id": "3a212ae49b3250a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(\"/Users/baoshui/data/Evol-Instruct-Code-80k-v1/EvolInstruct-Code-80k.json\", 'r', encoding='utf-8') as input_file, open(target_jsonl, 'a', encoding='utf-8') as output_file:\n",
    "    data = json.load(input_file)\n",
    "    for d in data:\n",
    "        new_data = {\n",
    "            \"query\": d[\"instruction\"],\n",
    "            \"answer\": d[\"output\"],\n",
    "            \"resource\": \"Python-Code-23k-ShareGPT\",\n",
    "            \"lang\": \"python\"\n",
    "        }\n",
    "        output_file.write(json.dumps(new_data, ensure_ascii=False) + '\\n')"
   ],
   "id": "58cc746b655a4ece",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 打印当前的数据重量\n",
    "def print_count_data():\n",
    "    with open(target_jsonl, 'r', encoding='utf-8') as output_file:\n",
    "        return sum(1 for _ in output_file)\n",
    "    \n",
    "print_count_data()"
   ],
   "id": "59bcce3eaa99c4d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "至此，已经完成了我们的第一步操作，合并4个数据集，得到共287k条数据。\n",
    "\n",
    "第二步，对所有的题目调用 `Qwen-72B-Chat` 进行打分，选择的分为4或5的题目，得到共156k条数据。\n",
    "\n",
    "第三步，使用Bert模型的嵌入和k-最近邻算法将相似的单轮查询-响应对合并形成多轮对话，最多选3个，也就是最多构建四轮对话。"
   ],
   "id": "9c7d98b8c9dbf2b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import random\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "source_pair = []\n",
    "with open(target_jsonl, 'r', encoding='utf-8') as output_file:\n",
    "    for line in output_file[0:100]: # 选前100条做测试\n",
    "        source_pair.append(json.loads(line))\n",
    "    \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = BertModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "# 获取查询的Bert Embedding\n",
    "def get_bert_embeddings(queries):\n",
    "    encoded_input = tokenizer(queries, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_input)\n",
    "    # 获取句子级别的嵌入\n",
    "    embeddings = output.last_hidden_state.mean(dim=1)\n",
    "    return embeddings\n",
    "\n",
    "# 获取所有查询的向量化表示\n",
    "queries = [pair[\"query\"] for pair in source_pair]\n",
    "embeddings = get_bert_embeddings(queries)\n",
    "\n",
    "# 使用k-最近邻算法找到每个查询的四个最接近的邻居\n",
    "nbrs = NearestNeighbors(n_neighbors=4, algorithm='auto').fit(embeddings)\n",
    "distances, indices = nbrs.kneighbors(embeddings)\n",
    "\n",
    "# 生成多轮对话数据\n",
    "multi_turn_conversation = []\n",
    "used_indices = set()\n",
    "\n",
    "for i, neighbors in enumerate(indices):\n",
    "    if i in used_indices:\n",
    "        continue\n",
    "    ns = [n for n in neighbors if n != i and n not in used_indices]\n",
    "    if len(ns) >= 2:\n",
    "        selected_neighbors = random.sample(ns, 2)\n",
    "        conversation = [source_pair[i]]\n",
    "        conversation.append([source_pair[n] for n in selected_neighbors])\n",
    "        multi_turn_conversation.append(conversation)\n",
    "        used_indices.update(selected_neighbors)\n",
    "        \n",
    "    used_indices.add(i)\n",
    "\n",
    "print(f\"生成了{len(multi_turn_conversation)}个多轮对话实例。\")"
   ],
   "id": "86844b050b4a0cdb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "204cbea36735c772",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
