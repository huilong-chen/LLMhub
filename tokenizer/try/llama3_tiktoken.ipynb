{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "从 Llama3 开始，Meta使用的分词技术从 sentencepiece 迁移到了 tiktoken\n",
    "\n",
    "sentencepiece和tiktoken的区别：\n",
    "1. sentencepiece\n",
    "   1. 预设一个vocab size \n",
    "   2. 将语料中的所有单个字符添加到词表\n",
    "   3. 通过一个循环来将词表扩展到指定的大小，每一轮都会新增一个token。新增一个token的原则：选择通过已有编码下token pair频率最高的这一对作为新增加的token。(可以想像这样一来某些单词里面常见的字母组合如 th、ng 什么的会被慢慢的用来扩充 vocab，继而更长的组合如 ing、scr 等。这样一来如此编码可以处理一些语料中没有看见过的单词） \n",
    "   4. 在编码字符串的时候可以使用greedy match，与最长的token进行匹配。 \n",
    "   5. 本质：带有字节回退的字符级bpe。基础是字符，未登录词回退为字符。\n",
    "2. tiktoken\n",
    "   1. 算法原理与sentencepiece基本一致。 \n",
    "   2. 区别在于，tiktoken认为基础的token为字节。tiktoken处理的文本为字节的序列，而非字符的序列。 \n",
    "   3. sentencepiece对字符进行操作，而非对UTF-8编码进行操作。例如\"你好\"是2个字符，但是是6个字节b\"\\xe4\\xbd\\xa0\\xe5\\xa5\\xbd\"。假如\"你好\"这个token既在sentenpiece的词表中，也在tiktoken的词表中，那么sentencepiece需要1次merge操作(\"你\", \"好\")，而在tiktoken词表中，则需要5次merge(b\"\\xe4\", b\"\\xbd\"), (b\"\\xe4\\xbd\", b\"\\xa0\"), (b\"\\xe5\", b\"\\xa5\"), (b\"\\xe5\\xa5\", \"\\xbd\"), (b\"\\xe4\\xbd\\xa0\", \"\\xe5\\xa5\\xbd\")\n",
    "   4. tiktoken和sentencepiece对未登录词的操作不同。假设\"佰\"( b\"\\xe4\\xbd\\xb0\") 不在词表中。则sentencepiece会回退产生序列(\"<0xE4>\", \"<0xBD>\", \"<0xB0>\")，而tiktoken可能会产生序列(b\"\\xe4\\xbd\", b\"\\xb0\")\n",
    "   5. sentencepiece使用的训练数据是一行一行的文本，但是tiktoken是将整个训练语料视作一个超长的字符串。所以sentencepiece训练出来的词表，每个换行(\\n)都是一个单独的token(<0x0A>)。但是tiktoken训练出来的词表，存在将多个换行拼成一个token。\n"
   ],
   "id": "8658ea4ad073a883"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "text = \"你好\"\n",
    "utf8_bytes = text.encode('utf-8')\n",
    "print(utf8_bytes)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "text = \"佰\"\n",
    "utf8_bytes = text.encode('utf-8')\n",
    "print(utf8_bytes)"
   ],
   "id": "be9536edfaccdabd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bytes = b'\\xe4\\xbd\\xb0'\n",
    "text = bytes.decode('utf-8')\n",
    "print(text)"
   ],
   "id": "fcad1e148962853c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "参考 Qwen 分词器的构成：\n",
    "\n",
    "![img_1.png](../images/img_qwen_tokenize.png)"
   ],
   "id": "69d98268467baa92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer_qwen2 = AutoTokenizer.from_pretrained(\"../model/Qwen2-7B\")\n",
    "\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")"
   ],
   "id": "c9157b5bd752000e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(tokenizer_qwen2.encode(\"00\"))\n",
    "print(enc.encode(\"00\"))\n",
    "\n",
    "print(tokenizer_qwen2.encode(\"ith\"))\n",
    "print(enc.encode(\"ith\"))\n",
    "\n",
    "print(tokenizer_qwen2.decode([411]))\n",
    "print(enc.encode(\"out\"))"
   ],
   "id": "64460701488812ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "因为从token id410开始，gpt4的词表和qwen的词表产生了错位，且随着token id逐渐增加，错位也逐渐增加，似乎是qwen有意剔除了某些token。\n",
    "\n",
    "目前meta官方并没有说明他们是如何得到128K词表的，但是，基本可以认定，他们生成词表的方式和qwen类似，都是在gpt4 10w词表的基础上，合并sentencepiece训练得到的词表进行的扩充。\n",
    "\n",
    "证明如下："
   ],
   "id": "aa108157dfff9862"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenizer_llama3 = AutoTokenizer.from_pretrained(\"../model/Meta-Llama-3-8B\")\n",
    "print(len(tokenizer_llama3))\n",
    "\n",
    "for i in range(100000):\n",
    "    if enc.decode([i]) != tokenizer_llama3.decode([i], clean_up_tokenization_spaces=False):\n",
    "        print(i)\n",
    "        print(enc.decode([i]))\n",
    "        print(tokenizer_llama3.decode([i]))\n",
    "        \n",
    "print(\"done\")"
   ],
   "id": "7c52c02133c3d31",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "尝试进行词表合并，将 llama3 的词表与 chatglm3 的词表合并。\n",
    "\n",
    "在 llama3 的模型文件中，只提供了fast版本的tokenizer，词表文件是 tokenizer.json，没有提供slow版本的tokenizer（也就是vocab.txt）\n",
    "\n",
    "而在 chatglm3 的模型文件中，提供的是 tokenizer.model"
   ],
   "id": "f0746ab77de3e9c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenizer_chatglm = AutoTokenizer.from_pretrained(\"../model/chatglm3-6b\", trust_remote_code=True)\n",
    "print(len(tokenizer_chatglm))\n",
    "print(tokenizer_chatglm.added_tokens_decoder)"
   ],
   "id": "631e30b95ce22194",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "合并按照下列规则：\n",
    "\n",
    "- 0-127999: 原始llama3词表，总计128000个。llama3词表中128000之后是added_tokens。\n",
    "- 127999 - x：chatglm3词表中的中文token，并与前128000个token去重。\n",
    "- x - y: 以added_tokens形式添加的原chatglm3中的特殊字符"
   ],
   "id": "eb828d86838236d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import copy\n",
    "with open(\"../model/Meta-Llama-3-8B/tokenizer.json\") as f:\n",
    "    llama3_tokenizer_json = json.load(f)\n",
    "with open(\"../model/Meta-Llama-3-8B/config.json\") as f:\n",
    "    llama3_config_json = json.load(f)\n",
    "with open(\"../model/Meta-Llama-3-8B/tokenizer_config.json\") as f:\n",
    "    llama3_tokenizer_config = json.load(f)\n",
    "with open(\"../model/Meta-Llama-3-8B/special_tokens_map.json\") as f:\n",
    "    llama3_special_tokens_map = json.load(f)\n",
    "with open(\"../model/Meta-Llama-3-8B/generation_config.json\") as f:\n",
    "    llama3_generation_config = json.load(f)"
   ],
   "id": "d3b1102ab55fd38f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "从指定路径读取 SentencePiece 分词模型文件。该模型文件包含了分词器的训练结果，包括子词单元及其频率或概率等信息。\n",
    "\n",
    "将模型文件解析为 ModelProto 实例，以便后续可以通过该实例访问模型中的详细信息，比如子词词典、词频等。\n",
    "\n"
   ],
   "id": "3c3ef0336967489f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sentencepiece.sentencepiece_model_pb2 as model\n",
    "chatglm3_tokenizer_sp = model.ModelProto()\n",
    "chatglm3_tokenizer_sp.ParseFromString(open(\"../model/chatglm3-6b/tokenizer.model\", \"rb\").read())\n",
    "\n",
    "vocab_size = len(chatglm3_tokenizer_sp.pieces)\n",
    "print(vocab_size)\n",
    "\n",
    "vocab_list = [piece.piece for piece in chatglm3_tokenizer_sp.pieces]\n",
    "vocab_file_path = \"../model/chatglm3-6b/vocab.txt\"\n",
    "\n",
    "with open(vocab_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for token in vocab_list:\n",
    "        f.write(token + \"\\n\")\n",
    "\n",
    "print(f\"词汇表已保存到 {vocab_file_path}\")"
   ],
   "id": "db642e1a16566a14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from hanzidentifier import is_simplified\n",
    "def is_contain_chinese(word):\n",
    "    \"\"\"\n",
    "    判断字符串是否包含中文字符\n",
    "    :param word: 字符串\n",
    "    :return: 布尔值，True表示包含中文，False表示不包含中文\n",
    "    \"\"\"\n",
    "\n",
    "    for char in word:\n",
    "        if is_simplified(char):\n",
    "            return True\n",
    "    return False"
   ],
   "id": "e8988a28e7930dac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(tokenizer_chatglm.special_tokens_map)",
   "id": "5b72b2c675082cab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T15:53:57.295686Z",
     "start_time": "2024-07-23T15:53:57.256269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chatglm3_special_tokens = [\"<|begin_of_text|>\", \"<|end_of_text|>\", \"<|start_header_id|>\", \"<|end_header_id|>\", \"<|eot_id|>\"]\n",
    "chatglm3_normal_tokens = []\n",
    "for index, piece in enumerate(chatglm3_tokenizer_sp.pieces):\n",
    "    if piece.type == 4:\n",
    "        if len(tokenizer_llama3.encode(piece.piece)) == 1:\n",
    "            pass\n",
    "        else:\n",
    "            if piece.piece not in {\"<human>\", \"<bot>\", \"<|im_start|>\", \"<|im_end|>\"}:\n",
    "                chatglm3_special_tokens.append(piece.piece)\n",
    "    elif piece.type == 1:\n",
    "        if index >= 32000:\n",
    "            chatglm3_normal_tokens.append(piece.piece)\n",
    "        else:\n",
    "            if is_simplified(piece.piece):\n",
    "                chatglm3_normal_tokens.append(piece.piece)\n",
    "\n",
    "print(len(chatglm3_special_tokens))\n",
    "print(len(chatglm3_normal_tokens))\n",
    "# print(chatglm3_normal_tokens)"
   ],
   "id": "4902777b991f7421",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "33160\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T15:52:15.430032Z",
     "start_time": "2024-07-23T15:52:15.363867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llama3_vocab = copy.deepcopy(llama3_tokenizer_json[\"model\"][\"vocab\"])\n",
    "last_token_index = 0\n",
    "for k, v in llama3_vocab.items():\n",
    "    if v > last_token_index:\n",
    "        last_token_index = v\n",
    "print(last_token_index)"
   ],
   "id": "aa601fcb08ebd167",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127999\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T16:02:34.296034Z",
     "start_time": "2024-07-23T16:02:34.288023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 创建一个从字节（bytes）到Unicode字符串的映射\n",
    "# Copied from transformers.models.gpt2.tokenization_gpt2.bytes_to_unicode\n",
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Returns list of utf-8 byte and a mapping to unicode strings. We specifically avoids mapping to whitespace/control\n",
    "    characters the bpe code barfs on.\n",
    "\n",
    "    The reversible bpe codes work on unicode strings. This means you need a large # of unicode characters in your vocab\n",
    "    if you want to avoid UNKs. When you're at something like a 10B token dataset you end up needing around 5K for\n",
    "    decent coverage. This is a significant percentage of your normal, say, 32K bpe vocab. To avoid that, we want lookup\n",
    "    tables between utf-8 bytes and unicode strings.\n",
    "    \"\"\"\n",
    "    bs = (\n",
    "            list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"¡\"), ord(\"¬\") + 1)) + list(\n",
    "        range(ord(\"®\"), ord(\"ÿ\") + 1))\n",
    "    )\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2 ** 8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2 ** 8 + n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))\n",
    "byte_encoder = bytes_to_unicode()\n",
    "print(byte_encoder)\n",
    "\n",
    "# Copied from transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer._tokenize\n",
    "def tokenize(token):\n",
    "    \"\"\"Tokenize a string.\"\"\"\n",
    "    token_list = [byte_encoder[b] for b in token.encode(\"utf-8\")]\n",
    "    token = \"\".join(token_list)  # Maps all our bytes to unicode strings, avoiding control tokens of the BPE (spaces in our case)\n",
    "    return token\n"
   ],
   "id": "9fa6df25c99220c5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{33: '!', 34: '\"', 35: '#', 36: '$', 37: '%', 38: '&', 39: \"'\", 40: '(', 41: ')', 42: '*', 43: '+', 44: ',', 45: '-', 46: '.', 47: '/', 48: '0', 49: '1', 50: '2', 51: '3', 52: '4', 53: '5', 54: '6', 55: '7', 56: '8', 57: '9', 58: ':', 59: ';', 60: '<', 61: '=', 62: '>', 63: '?', 64: '@', 65: 'A', 66: 'B', 67: 'C', 68: 'D', 69: 'E', 70: 'F', 71: 'G', 72: 'H', 73: 'I', 74: 'J', 75: 'K', 76: 'L', 77: 'M', 78: 'N', 79: 'O', 80: 'P', 81: 'Q', 82: 'R', 83: 'S', 84: 'T', 85: 'U', 86: 'V', 87: 'W', 88: 'X', 89: 'Y', 90: 'Z', 91: '[', 92: '\\\\', 93: ']', 94: '^', 95: '_', 96: '`', 97: 'a', 98: 'b', 99: 'c', 100: 'd', 101: 'e', 102: 'f', 103: 'g', 104: 'h', 105: 'i', 106: 'j', 107: 'k', 108: 'l', 109: 'm', 110: 'n', 111: 'o', 112: 'p', 113: 'q', 114: 'r', 115: 's', 116: 't', 117: 'u', 118: 'v', 119: 'w', 120: 'x', 121: 'y', 122: 'z', 123: '{', 124: '|', 125: '}', 126: '~', 161: '¡', 162: '¢', 163: '£', 164: '¤', 165: '¥', 166: '¦', 167: '§', 168: '¨', 169: '©', 170: 'ª', 171: '«', 172: '¬', 174: '®', 175: '¯', 176: '°', 177: '±', 178: '²', 179: '³', 180: '´', 181: 'µ', 182: '¶', 183: '·', 184: '¸', 185: '¹', 186: 'º', 187: '»', 188: '¼', 189: '½', 190: '¾', 191: '¿', 192: 'À', 193: 'Á', 194: 'Â', 195: 'Ã', 196: 'Ä', 197: 'Å', 198: 'Æ', 199: 'Ç', 200: 'È', 201: 'É', 202: 'Ê', 203: 'Ë', 204: 'Ì', 205: 'Í', 206: 'Î', 207: 'Ï', 208: 'Ð', 209: 'Ñ', 210: 'Ò', 211: 'Ó', 212: 'Ô', 213: 'Õ', 214: 'Ö', 215: '×', 216: 'Ø', 217: 'Ù', 218: 'Ú', 219: 'Û', 220: 'Ü', 221: 'Ý', 222: 'Þ', 223: 'ß', 224: 'à', 225: 'á', 226: 'â', 227: 'ã', 228: 'ä', 229: 'å', 230: 'æ', 231: 'ç', 232: 'è', 233: 'é', 234: 'ê', 235: 'ë', 236: 'ì', 237: 'í', 238: 'î', 239: 'ï', 240: 'ð', 241: 'ñ', 242: 'ò', 243: 'ó', 244: 'ô', 245: 'õ', 246: 'ö', 247: '÷', 248: 'ø', 249: 'ù', 250: 'ú', 251: 'û', 252: 'ü', 253: 'ý', 254: 'þ', 255: 'ÿ', 0: 'Ā', 1: 'ā', 2: 'Ă', 3: 'ă', 4: 'Ą', 5: 'ą', 6: 'Ć', 7: 'ć', 8: 'Ĉ', 9: 'ĉ', 10: 'Ċ', 11: 'ċ', 12: 'Č', 13: 'č', 14: 'Ď', 15: 'ď', 16: 'Đ', 17: 'đ', 18: 'Ē', 19: 'ē', 20: 'Ĕ', 21: 'ĕ', 22: 'Ė', 23: 'ė', 24: 'Ę', 25: 'ę', 26: 'Ě', 27: 'ě', 28: 'Ĝ', 29: 'ĝ', 30: 'Ğ', 31: 'ğ', 32: 'Ġ', 127: 'ġ', 128: 'Ģ', 129: 'ģ', 130: 'Ĥ', 131: 'ĥ', 132: 'Ħ', 133: 'ħ', 134: 'Ĩ', 135: 'ĩ', 136: 'Ī', 137: 'ī', 138: 'Ĭ', 139: 'ĭ', 140: 'Į', 141: 'į', 142: 'İ', 143: 'ı', 144: 'Ĳ', 145: 'ĳ', 146: 'Ĵ', 147: 'ĵ', 148: 'Ķ', 149: 'ķ', 150: 'ĸ', 151: 'Ĺ', 152: 'ĺ', 153: 'Ļ', 154: 'ļ', 155: 'Ľ', 156: 'ľ', 157: 'Ŀ', 158: 'ŀ', 159: 'Ł', 160: 'ł', 173: 'Ń'}\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T16:02:38.584694Z",
     "start_time": "2024-07-23T16:02:38.579139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_merges(vocab):\n",
    "    \"\"\"\n",
    "    By default will return vocab and merges with respect to their order, by sending `vocab_scores` we're going to\n",
    "    order the merges with respect to the piece scores instead.\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_scores, reverse = vocab, False\n",
    "    # Merges\n",
    "    merges = []\n",
    "    \n",
    "    for merge, piece_score in vocab_scores.items():\n",
    "        local = []\n",
    "        for index in range(1, len(merge)):\n",
    "            piece_l, piece_r = merge[:index], merge[index:]\n",
    "            if piece_l in vocab and piece_r in vocab:\n",
    "                local.append((piece_l, piece_r, piece_score))\n",
    "        local = sorted(local, key=lambda x: (vocab[x[0]], vocab[x[1]]))\n",
    "        merges.extend(local)\n",
    "\n",
    "    merges = sorted(merges, key=lambda val: val[2], reverse=reverse)\n",
    "    merges = [\" \".join([val[0], val[1]]) for val in merges]\n",
    "    return merges"
   ],
   "id": "57dd9ac8da6948d",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T16:07:32.739568Z",
     "start_time": "2024-07-23T16:07:32.569916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for token in chatglm3_normal_tokens:\n",
    "    token_key = tokenize(token)\n",
    "    # 特殊处理，如果一个token无法把它切成当前token，则没有存在的必要\n",
    "    if len(tokenizer_chatglm.tokenize(token)) == 3:\n",
    "        # print(token)\n",
    "        extra_token_key = token_key[:2]\n",
    "        if extra_token_key not in llama3_vocab:\n",
    "            last_token_index += 1\n",
    "            llama3_vocab[extra_token_key] = last_token_index\n",
    "    if token_key not in llama3_vocab:\n",
    "        last_token_index += 1\n",
    "        llama3_vocab[token_key] = last_token_index\n",
    "print(last_token_index)"
   ],
   "id": "28c5d60fdad6a3d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156439\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T16:06:41.142227Z",
     "start_time": "2024-07-23T16:06:41.137416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "token = \"新华社\"\n",
    "print(tokenizer_chatglm.tokenize(token))\n",
    "print(tokenize(token))\n",
    "\n",
    "token = \"新型冠状病毒\"\n",
    "print(tokenizer_chatglm.tokenize(token))\n",
    "print(tokenize(token))"
   ],
   "id": "a6fce98e7a6640d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁新', '华', '社']\n",
      "æĸ°åįİç¤¾\n",
      "['▁新', '型', '冠状病毒']\n",
      "æĸ°åŀĭåĨłçĬ¶çĹħæ¯Ĵ\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T16:09:37.529029Z",
     "start_time": "2024-07-23T16:09:37.522082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(tokenizer_chatglm.eos_token)\n",
    "print(tokenizer_chatglm.eos_token_id)\n",
    "\n",
    "print(tokenizer_llama3.eos_token)\n",
    "print(tokenizer_llama3.eos_token_id)"
   ],
   "id": "1c97b58ed60a42e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>\n",
      "2\n",
      "<|end_of_text|>\n",
      "128001\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d1c81635361d83d2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
