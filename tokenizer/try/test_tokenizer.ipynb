{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-23T14:46:23.474967Z",
     "start_time": "2024-07-23T14:46:23.465115Z"
    }
   },
   "source": [
    "import os\n",
    "os.getcwd()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/baoshui/code/LLMhub/tokenizer'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T14:46:51.587957Z",
     "start_time": "2024-07-23T14:46:51.539370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenlzier1 = AutoTokenizer.from_pretrained(\"./kanyun_tokenizer\", use_fast=True, trust_remote_code=True)\n",
    "print(tokenlzier1)\n",
    "print(len(tokenlzier1))\n",
    "print(tokenlzier1.special_tokens_map)\n",
    "# for i in tokenlzier1.added_tokens_decoder:\n",
    "#     print(i)"
   ],
   "id": "c4707de6cb860aee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KanyunTokenizerFast(name_or_path='./kanyun_tokenizer', vocab_size=52273, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t51979: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "52273\n",
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "text = \"Nice to meet you! \\n你好，我是小学生\"\n",
    "print(tokenlzier1.encode(text))\n",
    "d = [tokenlzier1.decode(i) for i in tokenlzier1.encode(text)]\n",
    "print(d)"
   ],
   "id": "3a18fc3ae4fa4039",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenlzier2 = AutoTokenizer.from_pretrained(\"../kanyun_tokenizer_tk\", use_fast=True, trust_remote_code=True)\n",
    "# print(tokenlzier2)\n",
    "print(len(tokenlzier2))\n",
    "print(tokenlzier2.special_tokens_map)\n",
    "print(tokenlzier2.added_tokens_decoder)"
   ],
   "id": "df4502acd38cf34d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(tokenlzier2.encode(text))\n",
    "d = [tokenlzier2.decode(i) for i in tokenlzier2.encode(text)]\n",
    "print(d)"
   ],
   "id": "ffe8cc2829caa995",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenlzier3 = AutoTokenizer.from_pretrained(\"../kanyun_tokenizer_tk_fix\", use_fast=True, trust_remote_code=True)\n",
    "# print(tokenlzier3)\n",
    "print(len(tokenlzier3))\n",
    "print(tokenlzier3.special_tokens_map)\n",
    "print(tokenlzier3.added_tokens_decoder)"
   ],
   "id": "26863264163c89d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(tokenlzier3.encode(text))\n",
    "d = [tokenlzier3.decode(i) for i in tokenlzier3.encode(text)]\n",
    "print(d)"
   ],
   "id": "8ac5aede5c078899",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenizer_qwen = AutoTokenizer.from_pretrained(\"/Users/baoshui/code/LLMhub/model/Qwen2-7B\", use_fast=True, trust_remote_code=True)\n",
    "print(len(tokenizer_qwen))\n",
    "print(tokenizer_qwen.special_tokens_map)"
   ],
   "id": "dc2b886ff4b739b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenizer_gpt = AutoTokenizer.from_pretrained(\"/Users/baoshui/code/LLMhub/tokenizer/gpt-3.5-turbo\", use_fast=True, trust_remote_code=True)\n",
    "print(len(tokenizer_gpt))"
   ],
   "id": "e6dccbbcdd48b97e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T14:42:42.698778Z",
     "start_time": "2024-07-23T14:42:41.245140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer_gpt4o = AutoTokenizer.from_pretrained(\"/Users/baoshui/code/LLMhub/tokenizer/gpt-4o\", use_fast=True, trust_remote_code=True)\n",
    "print(len(tokenizer_gpt4o))\n"
   ],
   "id": "51432ae0a7028b8f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")"
   ],
   "id": "d86568c9b3af6d2a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "enc.n_vocab",
   "id": "c15ceff775f533fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i in range(enc.n_vocab):\n",
    "    print(enc.decode([i]))"
   ],
   "id": "2d8bfdea76bed5fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T15:28:26.011582Z",
     "start_time": "2024-07-22T15:28:25.997908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "t = \"国网天津宁河电力公司以全面实现单位生产经营任务和提高队伍素质为目的,将公司生产经营管理目标和党建工作目标进行有机融合,形成涵盖安全生产、经营管理、党建思想政治工作等要求在内的党政统一目标管理体系。\"\n",
    "d = [tokenlzier1.decode(i) for i in tokenlzier1.encode(t)]\n",
    "print(d)\n"
   ],
   "id": "80e93e02d9e58c86",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '', '国', '网', '天津', '宁', '河', '电力', '公司', '以', '全面', '实现', '单位', '生产', '经营', '任务', '和', '提高', '队伍', '素质', '为', '目的', ',', '将', '公司', '生产', '经营', '管理', '目标', '和', '党建', '工作', '目标', '进行', '有机', '融合', ',', '形成', '涵盖', '安全', '生产', '、', '经营', '管理', '、', '党建', '思想', '政治', '工作', '等', '要求', '在', '内的', '党政', '统一', '目标', '管理', '体系', '。']\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T15:29:07.838635Z",
     "start_time": "2024-07-22T15:29:07.826585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "d = [tokenlzier3.decode(i) for i in tokenlzier3.encode(t)]\n",
    "print(d)"
   ],
   "id": "ddfa0e4bdbe362a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['国', '网', '天津', '宁', '河', '电力', '公司', '以', '全面', '实现', '单位', '生产', '经营', '任务', '和', '提高', '队伍', '素质', '为', '目的', ',', '将', '公司', '生产', '经营', '管理', '目标', '和', '党建', '工作', '目标', '进行', '有机', '融合', ',', '形成', '涵盖', '安全', '生产', '、', '经营', '管理', '、', '党建', '思想', '政治', '工作', '等', '要求', '在内', '的', '党政', '统一', '目标', '管理', '体系', '。']\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T15:30:46.268279Z",
     "start_time": "2024-07-22T15:30:46.059662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "t = \"国网天津宁河电力公司以全面实现单位生产经营任务和提高队伍素质为目的,将公司生产经营管理目标和党建工作目标进行有机融合,形成涵盖安全生产、经营管理、党建思想政治工作等要求在内的党政统一目标管理体系。\"\n",
    "tokenizer_llama3 = AutoTokenizer.from_pretrained(\"../model/Meta-Llama-3-8B\")\n",
    "d = [tokenizer_llama3.decode(i) for i in tokenizer_llama3.encode(t)]\n",
    "print(d)"
   ],
   "id": "b0f5fc309da048f0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['国', '网', '天', '津', '宁', '河', '电', '力', '公司', '以', '全面', '实现', '单位', '生产', '经营', '任务', '和', '提高', '队', '伍', '素', '质', '为', '目的', ',', '将', '公司', '生产', '经营', '管理', '目标', '和', '党', '建', '工作', '目标', '进行', '有', '机', '融', '合', ',', '形成', '�', '�', '盖', '安全', '生产', '、', '经营', '管理', '、', '党', '建', '思想', '政治', '工作', '等', '要求', '在', '内', '的', '党', '政', '统', '一', '目标', '管理', '体系', '。']\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T15:00:30.760188Z",
     "start_time": "2024-07-23T15:00:30.755776Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"\\n\"\n",
    "utf8_bytes = text.encode('utf-8')\n",
    "print(utf8_bytes)"
   ],
   "id": "b137080a3e70f1d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\n'\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T08:54:49.939020Z",
     "start_time": "2024-07-28T08:54:49.798733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from bytepiece import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer('../bytepiece.model')\n",
    "text = '今天天气不错'\n",
    "\n",
    "tokens = tokenizer.tokenize(text)  # 返回bytes的list\n",
    "print(b' '.join(tokens).decode(errors='ignore'))  # 可视化分词结果\n",
    "\n",
    "ids = tokenizer.encode(text)  # 返回tokens对应的ids\n",
    "print(tokenizer.decode(ids))  # 重新将ids解码为unicode文本\n",
    "ids = tokenizer.encode(text, iter=True)  # 返回ids的generator\n",
    "\n",
    "tokens = tokenizer.tokenize(text, alpha=0.2)  # 随机tokenize\n",
    "print(b' '.join(tokens).decode(errors='ignore'))  # 可视化分词结果"
   ],
   "id": "f30748035d8576ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天 天气 不错\n",
      "今天天气不错\n",
      "今天 天气 不 错\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T08:59:44.485153Z",
     "start_time": "2024-07-28T08:59:44.393147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from bytepiece import Tokenizer\n",
    "tokenizer1 = Tokenizer('../bytepiece.model')\n",
    "tokenizer1.convert_to_sentencepiece('../bytepiece_sp.model')\n",
    "\n",
    "import sentencepiece as spm\n",
    "tokenizer2 = spm.SentencePieceProcessor('../bytepiece_sp.model')\n",
    "\n",
    "print(tokenizer1.encode('今天天气不错'))\n",
    "print(tokenizer2.encode('今天天气不错'))"
   ],
   "id": "dfa0044c07163171",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10453, 11345, 10214]\n",
      "[10453, 11345, 10214]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7b3579ed4a30aa57"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
